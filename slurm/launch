#! /bin/bash

# --- usage 
programname=$0

function usage {
    echo "usage: $programname sequences [--<kwarg> <value>]" 
    echo "  --cpus           number of cpus to request"
    echo "  --gpus           number of gpus to request"
    echo "  --mem            amount of memory (in GB) to request" 
    echo "  --who            email to report to upon slurm job completion"
    echo "  --time           job time limit (hours)"
    exit 1
}

# --- parse the cli arguments
num_nodes=${num_nodes:-1}
num_tasks=${num_tasks:-1}
time=${time:-12:00:00}
cpus=${cpus:-16}
gpus=${gpus:-8}
mem=${mem:-128}
who=${who:-njkrichardson@princeton.edu}

while [ $# -gt 0 ]; do

   if [[ $1 == *"--"* ]]; then
        param="${1/--/}"
        declare $param="$2"
   fi

  shift
done

# --- verbose 
echo "Deploying slurm job to the cluster."
echo "-----------------------------------------------" 
echo "Number of cpus: $cpus" 
echo "Number of gpus: $gpus" 
echo "Job time limit: $time" 
echo "Memory requested: ${mem} GB" 
echo "Contact user: ${who}" 
echo "-----------------------------------------------" 

# --- render 
sbatch <<EOT
#!/bin/bash
#
# --- admin
#SBATCH --job-name=test
#SBATCH --partition=lips
#SBATCH --mail-user=$who
#SBATCH --mail-type=end
#SBATCH --time=$time
#
# --- resources 
#SBATCH --nodes=$num_nodes
#SBATCH --ntasks=$num_tasks
#SBATCH --cpus-per-task=$cpus
#SBATCH --gres=gpu:$gpus
#SBATCH --mem=${mem}gb 

set -um; 

/n/fs/jaxplace/miniconda3/envs/diffusion/bin/python3 /n/fs/jaxplace/projects/diffusion/experiments/train_distributed.py --batch-size=2 --num-steps=50_000 --distributed --num-to-sample 4; 

exit 0
EOT
